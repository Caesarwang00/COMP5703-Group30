# -*- coding: utf-8 -*-
"""
train_search.py â€” è‡ªåŠ¨è¶…å‚æœç´¢ (GraphSAGE)
æœç´¢ hid / lr / dropout / weight_decay ç»„åˆï¼Œå¹¶ä¿å­˜æ¯ç»„çš„æœ€ä½³ Val/Test å‡†ç¡®ç‡ã€‚

éœ€è¦çš„æ–‡ä»¶ï¼š
  - multiomics_fused_features.tsv   (æ ·æœ¬ Ã— ç‰¹å¾)
  - edge_list.csv                   (ä¸¤åˆ—ï¼šsource/target æˆ–ç±»ä¼¼å‘½å)
  - subtype_labels.tsv              (è‡³å°‘å« sample / subtype æˆ– sampleid / geneexp_subtype)

è¾“å‡ºï¼š
  - search_results.csv              (æ¯ç»„è¶…å‚çš„ best_val / best_test)
  - æ§åˆ¶å°æœ€åæ‰“å°æœ€ä¼˜è¶…å‚
"""

import os
import itertools
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from torch import nn
from torch_geometric.nn import SAGEConv
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.preprocessing import LabelEncoder

SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)

# -------------------------
# è¯»å–ç‰¹å¾
# -------------------------
FEAT = "../data_RNA/multiomics_fused_features.tsv"
if not os.path.exists(FEAT):
    raise FileNotFoundError(f"ç¼ºå°‘ {FEAT}")

X = pd.read_csv(FEAT, sep="\t", index_col=0)
X.index = X.index.astype(str).str.strip()
node_ids = X.index.tolist()
x = torch.tensor(X.values, dtype=torch.float32)
id2idx = {sid: i for i, sid in enumerate(node_ids)}
print(f"âœ… è¯»å–ç‰¹å¾ï¼šsamples={x.shape[0]}, dims={x.shape[1]}")

# -------------------------
# è¯»å–è¾¹ (æ›´å¥å£®ï¼šè‡ªåŠ¨è¯†åˆ«åˆ—åï¼Œæ˜ å°„å dropnaï¼Œä¿è¯ src/dst ç­‰é•¿)
# -------------------------
EL = "../data_RNA/edge_list.csv"
if not os.path.exists(EL):
    raise FileNotFoundError(f"ç¼ºå°‘ {EL}")

edges = pd.read_csv(EL)
edges.columns = [str(c).strip().lower() for c in edges.columns]

# è‡ªåŠ¨è¯†åˆ«ä¸¤åˆ—
cand_src = [c for c in ["source","src","from","u","node1"] if c in edges.columns]
cand_dst = [c for c in ["target","dst","to","v","node2"]  if c in edges.columns]
if not cand_src or not cand_dst:
    if edges.shape[1] >= 2:
        edges = edges.iloc[:, :2]
        edges.columns = ["source","target"]
    else:
        raise ValueError("edge_list.csv è‡³å°‘éœ€è¦ä¸¤åˆ—ï¼ˆsource/targetï¼‰")

edges = edges[[cand_src[0] if cand_src else "source",
               cand_dst[0]  if cand_dst  else "target"]].copy()
edges.columns = ["source","target"]

# æ˜ å°„åˆ°èŠ‚ç‚¹ç´¢å¼•ï¼Œå¹¶ä¸¢å¼ƒæœªå¯¹é½çš„è¡Œ
edges["source"] = edges["source"].astype(str).str.strip().map(id2idx)
edges["target"] = edges["target"].astype(str).str.strip().map(id2idx)
edges = edges.dropna()
src = edges["source"].astype(int).to_numpy()
dst = edges["target"].astype(int).to_numpy()

# ç»„è£… edge_indexï¼ˆä¿è¯ä¸¤ä¸ªæ•°ç»„ç­‰é•¿ï¼‰
if len(src) == 0 or len(dst) == 0 or len(src) != len(dst):
    raise ValueError(f"è¾¹æ•°æ®ä¸åˆæ³•ï¼šsrc={len(src)} dst={len(dst)}ï¼ˆéœ€ç›¸ç­‰ä¸” >0ï¼‰")
edge_index = torch.tensor([src, dst], dtype=torch.long)
print(f"âœ… è¯»å–è¾¹ï¼šedges={edge_index.shape[1]}")

# -------------------------
# è¯»å–æ ‡ç­¾ & åˆ†å±‚åˆ’åˆ† (train/val/test = 70/15/15)
# -------------------------
LAB = "../data_RNA/subtype_labels.tsv"
if not os.path.exists(LAB):
    raise FileNotFoundError(f"ç¼ºå°‘ {LAB}")

labels = pd.read_csv(LAB, sep="\t")
labels.columns = [str(c).strip().lower() for c in labels.columns]
name_col = "sample" if "sample" in labels.columns else ("sampleid" if "sampleid" in labels.columns else None)
sub_col  = "subtype" if "subtype" in labels.columns else ("geneexp_subtype" if "geneexp_subtype" in labels.columns else None)
if name_col is None or sub_col is None:
    raise ValueError("æ ‡ç­¾æ–‡ä»¶éœ€è¦åŒ…å« sample/subtype æˆ– sampleid/geneexp_subtype ä¸¤åˆ—")

labels = labels.rename(columns={name_col: "sample", sub_col: "subtype"})
labels["sample"] = labels["sample"].astype(str).str.strip()
labels = labels.set_index("sample").reindex(node_ids)

keep = ~labels["subtype"].isna()
labeled_nodes = np.where(keep.values)[0]
if len(labeled_nodes) == 0:
    raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ ‡ç­¾ï¼ˆå…¨éƒ¨ä¸ºç¼ºå¤±ï¼‰")

le = LabelEncoder()
y_full = np.full(len(node_ids), -1, dtype=int)
y_full[labeled_nodes] = le.fit_transform(labels.loc[keep, "subtype"].values)
classes = list(le.classes_)
y = torch.tensor(y_full, dtype=torch.long)
print(f"âœ… æ ‡ç­¾ï¼š{len(classes)} ç±» â†’ {classes}")

# åˆ†å±‚åˆ’åˆ†
sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.30, random_state=SEED)
tr_idx, vt_idx = next(sss1.split(np.zeros(len(labeled_nodes)), y_full[labeled_nodes]))
sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.50, random_state=SEED)
v_idx, te_idx = next(sss2.split(np.zeros(len(vt_idx)), y_full[labeled_nodes][vt_idx]))

train_nodes = labeled_nodes[tr_idx]
val_nodes   = labeled_nodes[vt_idx[v_idx]]
test_nodes  = labeled_nodes[vt_idx[te_idx]]

n = len(node_ids)
train_mask = torch.zeros(n, dtype=torch.bool); train_mask[train_nodes] = True
val_mask   = torch.zeros(n, dtype=torch.bool); val_mask[val_nodes]    = True
test_mask  = torch.zeros(n, dtype=torch.bool); test_mask[test_nodes]  = True
print(f"âœ… åˆ’åˆ†ï¼štrain={train_mask.sum().item()}, val={val_mask.sum().item()}, test={test_mask.sum().item()}")

# -------------------------
# å®šä¹‰æ¨¡å‹ä¸å·¥å…·å‡½æ•°
# -------------------------
class SageNet(nn.Module):
    def __init__(self, in_ch, hid=64, out_ch=4, drop=0.5):
        super().__init__()
        self.conv1 = SAGEConv(in_ch, hid)
        self.bn1   = nn.BatchNorm1d(hid)
        self.conv2 = SAGEConv(hid, hid)
        self.bn2   = nn.BatchNorm1d(hid)
        self.out   = nn.Linear(hid, out_ch)
        self.drop  = drop
    def forward(self, x, edge_index):
        x = F.relu(self.bn1(self.conv1(x, edge_index)))
        x = F.dropout(x, p=self.drop, training=self.training)
        x = F.relu(self.bn2(self.conv2(x, edge_index)))
        x = F.dropout(x, p=self.drop, training=self.training)
        return self.out(x)

def accuracy(logits, labels, mask):
    if mask.sum() == 0: return float("nan")
    pred = logits[mask].argmax(dim=1)
    return (pred == labels[mask]).float().mean().item()

# -------------------------
# è¶…å‚ç½‘æ ¼
# -------------------------
param_grid = {
    "hid":  [48, 64],                           # è´´è¿‘ä½ æœ€ä½³çš„ 64ï¼Œä¹Ÿè¯• 48
    "lr":   [3e-4, 5e-4, 7e-4, 1e-3],           # ä»¥ 5e-4 ä¸ºä¸­å¿ƒçš„çª„åŸŸ
    "drop": [0.3, 0.4, 0.5],                    # å›´ç»• 0.4
    "wd":   [0.0, 1e-5, 5e-5, 1e-4],            # å…³é”®ï¼šåŠ å…¥ 0
}

combos = list(itertools.product(param_grid["hid"], param_grid["lr"], param_grid["drop"], param_grid["wd"]))

results = []
print(f"ğŸ” å¼€å§‹æœç´¢ï¼Œå…± {len(combos)} ç»„â€¦")

# -------------------------
# è®­ç»ƒå¾ªç¯ï¼ˆæ¯ç»„ 200 epochï¼Œå– best Val/Testï¼‰
# -------------------------
for hid, lr, drop, wd in combos:
    model = SageNet(x.shape[1], hid=hid, out_ch=len(classes), drop=drop)
    opt   = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)

    best_val, best_test = 0.0, 0.0
    for epoch in range(1, 201):
        model.train()
        opt.zero_grad()
        out = model(x, edge_index)
        loss = F.cross_entropy(out[train_mask], y[train_mask])
        loss.backward()
        opt.step()

        if epoch % 20 == 0:
            model.eval()
            with torch.no_grad():
                logits = model(x, edge_index)
                val_acc = accuracy(logits, y, val_mask)
                test_acc = accuracy(logits, y, test_mask)
                if val_acc > best_val:
                    best_val = val_acc
                    best_test = test_acc

    results.append({
        "hid": hid, "lr": lr, "drop": drop, "wd": wd,
        "best_val": float(best_val), "best_test": float(best_test)
    })
    print(f"âœ… hid={hid}, lr={lr}, drop={drop}, wd={wd} | Val={best_val:.3f} Test={best_test:.3f}")

# -------------------------
# ä¿å­˜ç»“æœå¹¶è¾“å‡ºæœ€ä¼˜ç»„åˆ
# -------------------------
df = pd.DataFrame(results)
df.to_csv("search_results.csv", index=False)
best = df.sort_values("best_val", ascending=False).iloc[0]
print("\nğŸ‰ æœ€ä¼˜è¶…å‚:")
print(best)



