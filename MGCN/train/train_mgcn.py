# -*- coding: utf-8 -*-
"""
train_mgcn_rna.py â€” æœ€ç»ˆç‰ˆ GraphSAGE è®­ç»ƒè„šæœ¬ï¼ˆæ”¯æŒå¤–éƒ¨ SEEDã€å¤šæ–‡ä»¶å¥å£®è¯»å–ã€æ—©åœã€ç±»åˆ«æƒé‡ã€GNNGuardï¼‰

è¾“å…¥ï¼ˆåŒç›®å½•ï¼‰ï¼š
  - multiomics_fused_features.tsv   è¡Œ=æ ·æœ¬ï¼›åˆ—=èåˆåçš„å¤šç»„å­¦ç‰¹å¾
  - edge_list.csv                   ä¸¤åˆ—ï¼ˆè‡ªåŠ¨è¯†åˆ« source/target/src/dst ç­‰å‘½åï¼‰
  - subtype_labels.tsv              è‡³å°‘åŒ…å« sample/subtypeï¼ˆæˆ– sampleid/geneexp_subtypeï¼‰

å¯é€‰ç¯å¢ƒå˜é‡ï¼š
  - SEEDï¼šéšæœºç§å­ï¼ˆé»˜è®¤ 42ï¼‰

è¾“å‡ºï¼š
  - node_embeddings.tsv             å…¨é‡æ ·æœ¬çš„èŠ‚ç‚¹è¡¨ç¤ºï¼ˆç”¨äºå¯è§†åŒ–/ä¸‹æ¸¸ä»»åŠ¡ï¼‰
  - predictions_test.csv            æµ‹è¯•é›†é¢„æµ‹æ˜ç»†ï¼ˆå«çœŸå®/é¢„æµ‹åŠæ¯ç±»æ¦‚ç‡ï¼‰
  - metrics_summary.txt             JSON æ–‡æœ¬ï¼ŒåŒ…å« Train/Val/Test æŒ‡æ ‡ã€åˆ†ç±»æŠ¥å‘Šã€æ··æ·†çŸ©é˜µ
"""

import os
import json
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from torch import nn
from torch_geometric.nn import SAGEConv
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix

# ========================
# éšæœºç§å­ï¼ˆå¯è¢«ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
# ========================
SEED = int(os.environ.get("SEED", "42"))
torch.manual_seed(SEED)
np.random.seed(SEED)

# ========================
# å›ºå®šæœ€ä¼˜è¶…å‚ï¼ˆæ¥è‡ªä½ çš„æœç´¢ï¼‰
# ========================
HID_DIM       = 64
LR            = 0.0005
DROPOUT       = 0.4
WEIGHT_DECAY  = 5e-4
MAX_EPOCHS    = 800
PATIENCE      = 30   # ä»¥éªŒè¯é›†å‡†ç¡®ç‡ä¸ºå‡†çš„æ—©åœè€å¿ƒ

# ========================
# è¯»å–ç‰¹å¾
# ========================
FEAT = "../data_RNA/multiomics_fused_features.tsv"
if not os.path.exists(FEAT):
    raise FileNotFoundError(f"ç¼ºå°‘ {FEAT}")
X = pd.read_csv(FEAT, sep="\t", index_col=0)
X.index = X.index.astype(str).str.strip()
node_ids = X.index.tolist()
x = torch.tensor(X.values, dtype=torch.float32)
id2idx = {sid: i for i, sid in enumerate(node_ids)}
print(f"âœ… è¯»å–ç‰¹å¾ï¼šsamples={x.shape[0]}, dims={x.shape[1]}")

# ========================
# è¯»å–è¾¹ï¼ˆè‡ªåŠ¨è¯†åˆ«åˆ—åï¼›æ˜ å°„å dropnaï¼›åŒ…å« GNNGuard é‡åŠ æƒï¼‰
# ========================
EL = "../data_RNA/edge_list.csv"
if not os.path.exists(EL):
    raise FileNotFoundError(f"ç¼ºå°‘ {EL}")
edges = pd.read_csv(EL)
edges.columns = [str(c).strip().lower() for c in edges.columns]
cand_src = [c for c in ["source","src","from","u","node1"] if c in edges.columns]
cand_dst = [c for c in ["target","dst","to","v","node2"]  if c in edges.columns]
if not cand_src or not cand_dst:
    if edges.shape[1] >= 2:
        edges = edges.iloc[:, :2]
        edges.columns = ["source","target"]
    else:
        raise ValueError("edge_list.csv è‡³å°‘éœ€è¦ä¸¤åˆ—ï¼ˆsource/targetï¼‰")
edges = edges[[cand_src[0] if cand_src else "source",
               cand_dst[0]  if cand_dst  else "target"]].copy()
edges.columns = ["source","target"]
edges["source"] = edges["source"].astype(str).str.strip().map(id2idx)
edges["target"] = edges["target"].astype(str).str.strip().map(id2idx)
edges = edges.dropna()
src = edges["source"].astype(int).to_numpy()
dst = edges["target"].astype(int).to_numpy()
if len(src) == 0 or len(src) != len(dst):
    raise ValueError(f"è¾¹æ•°æ®ä¸åˆæ³•ï¼šsrc={len(src)} dst={len(dst)}ï¼ˆéœ€ç›¸ç­‰ä¸”>0ï¼‰")

# æ›´å¿«åœ°åˆ›å»ºå¼ é‡ï¼Œé¿å… â€œlist of numpy.ndarrays is slowâ€ çš„å‘Šè­¦
edge_index = torch.from_numpy(np.vstack([src, dst]).astype(np.int64))
print(f"âœ… ä½¿ç”¨ edge_list.csv æ„å›¾ï¼šedges={edge_index.shape[1]}")

# ---- ç®€åŒ–ç‰ˆ GNNGuardï¼šæŒ‰ä½™å¼¦ç›¸ä¼¼åº¦é‡åŠ æƒå¹¶å¯¹æ¯ä¸ªèŠ‚ç‚¹ä¿ç•™ Top-p é‚»å±… ----
def gnnguard_reweight(x_feat: torch.Tensor, edge_index: torch.Tensor,
                      p_keep: float = 0.7, alpha: float = 2.0):
    row, col = edge_index
    h = F.normalize(x_feat, dim=1)                 # å½’ä¸€åŒ–ç‰¹å¾
    s = (h[row] * h[col]).sum(dim=1).clamp(min=0)  # ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæˆªè´Ÿï¼‰
    w = (s ** alpha)                                # å¹‚æ¬¡å¼ºåŒ–

    E = edge_index.size(1)
    keep = torch.zeros(E, dtype=torch.bool)
    idx_by_src = {}
    for e in range(E):
        i = int(row[e]); idx_by_src.setdefault(i, []).append(e)
    for i, eidxs in idx_by_src.items():
        k = max(1, int(len(eidxs) * p_keep))
        topk = torch.topk(w[eidxs], k).indices
        keep_eidx = [eidxs[int(t)] for t in topk]
        keep[keep_eidx] = True

    new_ei = edge_index[:, keep]
    new_w  = w[keep]
    # é•œåƒåå‘è¾¹ï¼ˆå¢å¼ºæ— å‘æ€§ï¼‰
    new_ei = torch.cat([new_ei, new_ei.flip(0)], dim=1)
    new_w  = torch.cat([new_w,  new_w], dim=0)
    return new_ei, new_w

edge_index, edge_weight = gnnguard_reweight(x, edge_index, p_keep=0.7, alpha=2.0)
print(f"âœ… GNNGuard åï¼šedges={edge_index.shape[1]}")

# ========================
# è¯»å–æ ‡ç­¾ & åˆ†å±‚åˆ’åˆ† (70/15/15)
# ========================
LAB = "../data_RNA/subtype_labels.tsv"
if not os.path.exists(LAB):
    raise FileNotFoundError(f"ç¼ºå°‘ {LAB}")
labels = pd.read_csv(LAB, sep="\t")
labels.columns = [str(c).strip().lower() for c in labels.columns]
name_col = "sample" if "sample" in labels.columns else ("sampleid" if "sampleid" in labels.columns else None)
sub_col  = "subtype" if "subtype" in labels.columns else ("geneexp_subtype" if "geneexp_subtype" in labels.columns else None)
if name_col is None or sub_col is None:
    raise ValueError("æ ‡ç­¾æ–‡ä»¶éœ€è¦åŒ…å« sample/subtype æˆ– sampleid/geneexp_subtype ä¸¤åˆ—")
labels = labels.rename(columns={name_col: "sample", sub_col: "subtype"})
labels["sample"] = labels["sample"].astype(str).str.strip()
labels = labels.set_index("sample").reindex(node_ids)

keep = ~labels["subtype"].isna()
labeled_nodes = np.where(keep.values)[0]
if len(labeled_nodes) == 0:
    raise ValueError("æ²¡æœ‰å¯ç”¨æ ‡ç­¾ï¼ˆå…¨éƒ¨ç¼ºå¤±ï¼‰")

le = LabelEncoder()
y_full = np.full(len(node_ids), -1, dtype=int)
y_full[labeled_nodes] = le.fit_transform(labels.loc[keep, "subtype"].values)
classes = list(le.classes_)
y = torch.tensor(y_full, dtype=torch.long)
print(f"âœ… è¯»å–æ ‡ç­¾ï¼š{len(classes)} ç±» â†’ {classes}")

# åˆ†å±‚åˆ’åˆ†ï¼ˆ70/15/15ï¼‰
sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.30, random_state=SEED)
tr_idx, vt_idx = next(sss1.split(np.zeros(len(labeled_nodes)), y_full[labeled_nodes]))
sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.50, random_state=SEED)
v_idx, te_idx = next(sss2.split(np.zeros(len(vt_idx)), y_full[labeled_nodes][vt_idx]))

train_nodes = labeled_nodes[tr_idx]
val_nodes   = labeled_nodes[vt_idx[v_idx]]
test_nodes  = labeled_nodes[vt_idx[te_idx]]

n = len(node_ids)
train_mask = torch.zeros(n, dtype=torch.bool); train_mask[train_nodes] = True
val_mask   = torch.zeros(n, dtype=torch.bool); val_mask[val_nodes]    = True
test_mask  = torch.zeros(n, dtype=torch.bool); test_mask[test_nodes]  = True
print(f"âœ… ç›‘ç£è®­ç»ƒï¼ˆåˆ†å±‚åˆ’åˆ†ï¼‰ï¼štrain={train_mask.sum().item()}, val={val_mask.sum().item()}, test={test_mask.sum().item()}")

# ç±»åˆ«æƒé‡ï¼ˆæŒ‰è®­ç»ƒé›†é¢‘æ¬¡çš„åæ¯”ï¼Œé˜²æ­¢é•¿å°¾ï¼‰
train_labels = y[train_mask].numpy()
cls, cnt = np.unique(train_labels, return_counts=True)
weights = np.zeros(len(classes), dtype=np.float32) + 1.0
for c, c_cnt in zip(cls, cnt):
    weights[c] = float(len(train_labels)) / (len(cls) * c_cnt)
class_weights = torch.tensor(weights, dtype=torch.float32)
print(f"âœ… ç±»åˆ«æƒé‡ï¼š{class_weights.numpy()}")

# ========================
# GraphSAGE æ¨¡å‹
# ========================
class SageNet(nn.Module):
    def __init__(self, in_ch, hid=48, out_ch=4, drop=0.6):
        super().__init__()
        self.conv1 = SAGEConv(in_ch, hid)
        self.bn1   = nn.BatchNorm1d(hid)
        self.conv2 = SAGEConv(hid, hid)
        self.bn2   = nn.BatchNorm1d(hid)
        self.out   = nn.Linear(hid, out_ch)
        self.drop  = drop
    def forward(self, x, edge_index, edge_weight=None):
        x = F.relu(self.bn1(self.conv1(x, edge_index)))
        x = F.dropout(x, p=self.drop, training=self.training)
        x = F.relu(self.bn2(self.conv2(x, edge_index)))
        x = F.dropout(x, p=self.drop, training=self.training)
        return self.out(x)
    def embeddings(self, x, edge_index):
        with torch.no_grad():
            h = F.relu(self.bn1(self.conv1(x, edge_index)))
            h = F.relu(self.bn2(self.conv2(h, edge_index)))
        return h

def accuracy(logits, labels, mask):
    if mask.sum()==0: return float("nan")
    pred = logits[mask].argmax(dim=1)
    return (pred == labels[mask]).float().mean().item()
# === æ–°å¢ï¼šé€ç±»å‡†ç¡®ç‡ ===
def per_class_accuracy(logits, labels, mask, classes):
    """
    å„ç±»åˆ«ä¸Šçš„â€œæ­£ç¡®ç‡â€ï¼ˆ= è¯¥ç±»è¢«åˆ¤å¯¹çš„æ¯”ä¾‹ï¼Œç­‰ä»·äº classification_report çš„ recallï¼‰ã€‚
    å½“æŸä¸ªåˆ‡åˆ†é‡Œæ²¡æœ‰è¯¥ç±»æ ·æœ¬æ—¶ï¼Œè¿”å› NaNã€‚
    """
    if mask.sum() == 0:
        return {c: float("nan") for c in classes}
    pred = logits[mask].argmax(dim=1).cpu().numpy()
    true = labels[mask].cpu().numpy()
    acc = {}
    for ci, cname in enumerate(classes):
        sel = (true == ci)
        if sel.sum() == 0:
            acc[cname] = float("nan")
        else:
            acc[cname] = float((pred[sel] == ci).mean())
    return acc

def _fmt_pc(pc_dict):
    import numpy as np
    return " | ".join(
        [f"{k}:{(v if not np.isnan(v) else float('nan')):.3f}" for k, v in pc_dict.items()]
    )

# ========================
# è®­ç»ƒï¼ˆæ—©åœï¼‰
# ========================
model = SageNet(x.shape[1], hid=HID_DIM, out_ch=len(classes), drop=DROPOUT)
opt   = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
best_val, best_state, wait = 0.0, None, 0

for epoch in range(1, MAX_EPOCHS+1):
    model.train()
    opt.zero_grad()
    out  = model(x, edge_index, None)
    loss = F.cross_entropy(out[train_mask], y[train_mask], weight=class_weights)
    loss.backward(); opt.step()

    if epoch % 10 == 0 or epoch == 1:
        model.eval()
        with torch.no_grad():
            logits = model(x, edge_index, None)
            tr = accuracy(logits, y, train_mask)
            va = accuracy(logits, y, val_mask)
            print(f"Epoch {epoch:03d} | Loss {loss.item():.4f} | Train {tr:.3f} | Val {va:.3f} | BestVal {best_val:.3f}")
            if va > best_val:
                best_val  = va
                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
                wait = 0
            else:
                wait += 1
                if wait >= PATIENCE:
                    print(f"â¹ æ—©åœäº Epoch {epoch}ï¼ˆBest Val={best_val:.3f}ï¼‰")
                    break

# åŠ è½½æœ€ä½³æƒé‡å¹¶æœ€ç»ˆè¯„ä¼°
if best_state is not None:
    model.load_state_dict(best_state)

model.eval()
with torch.no_grad():
    logits = model(x, edge_index, None)
    train_acc = accuracy(logits, y, train_mask)
    val_acc   = accuracy(logits, y, val_mask)
    test_acc  = accuracy(logits, y, test_mask)

    # é€ç±»å‡†ç¡®ç‡ï¼ˆ=å„ç±»åˆ«å¬å›ç‡ï¼‰
    pc_train = per_class_accuracy(logits, y, train_mask, classes)
    pc_val   = per_class_accuracy(logits, y, val_mask,   classes)
    pc_test  = per_class_accuracy(logits, y, test_mask,  classes)

print(f"âœ… æœ€ç»ˆè¯„ä¼° | Train {train_acc:.3f} | Val {val_acc:.3f} | Test {test_acc:.3f}")
print("ğŸ“Š å„ç±»å‡†ç¡®ç‡ï¼ˆTrainï¼‰:", _fmt_pc(pc_train))
print("ğŸ“Š å„ç±»å‡†ç¡®ç‡ï¼ˆValï¼‰  :", _fmt_pc(pc_val))
print("ğŸ“Š å„ç±»å‡†ç¡®ç‡ï¼ˆTestï¼‰ :", _fmt_pc(pc_test))


# ========================
# ä¿å­˜è¾“å‡º
# ========================
# èŠ‚ç‚¹åµŒå…¥
# emb = models.embeddings(x, edge_index).cpu().numpy()
# pd.DataFrame(emb, index=node_ids).to_csv("../other/node_embeddings.tsv", sep="\t")
# print("ğŸ’¾ å·²ä¿å­˜ï¼šnode_embeddings.tsv")
#
# # æµ‹è¯•é›†é¢„æµ‹æ˜ç»†ï¼ˆå«å„ç±»æ¦‚ç‡ï¼‰
# proba = F.softmax(logits, dim=1).cpu().numpy()
# pred  = logits.argmax(dim=1).cpu().numpy()
# inv_labels = np.array(classes)
#
# test_rows = np.where(test_mask.cpu().numpy())[0]
# pred_df = pd.DataFrame({
#     "sample":   [node_ids[i] for i in test_rows],
#     "true":     [inv_labels[y[i].item()]   for i in test_rows],
#     "pred":     [inv_labels[pred[i]]       for i in test_rows],
# })
# for ci, cname in enumerate(classes):
#     pred_df[f"prob_{cname}"] = proba[test_rows, ci]
# pred_df.to_csv("predictions_test.csv", index=False)
# print("ğŸ’¾ å·²ä¿å­˜ï¼špredictions_test.csv")
#
# # æ±‡æ€»æŒ‡æ ‡ + åˆ†ç±»æŠ¥å‘Š + æ··æ·†çŸ©é˜µï¼ˆæµ‹è¯•é›†ï¼‰
# y_true = [inv_labels[y[i].item()] for i in test_rows]
# y_pred = [inv_labels[pred[i]]     for i in test_rows]
# report = classification_report(y_true, y_pred, labels=classes, digits=3, output_dict=True)
# cm = confusion_matrix(y_true, y_pred, labels=classes).tolist()
#
# summary = {
#     "seed": SEED,
#     "classes": classes,
#     "train_acc": float(train_acc),
#     "val_acc":   float(val_acc),
#     "test_acc":  float(test_acc),
#     "classification_report": report,
#     "confusion_matrix": cm
# }
# with open("../other/metrics_summary.txt", "w", encoding="utf-8") as f:
#     f.write(json.dumps(summary, ensure_ascii=False, indent=2))
# print("ğŸ’¾ å·²ä¿å­˜ï¼šmetrics_summary.txt")
# print("ğŸ‰ å®Œæˆï¼")
